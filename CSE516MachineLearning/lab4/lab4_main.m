%Li 006704029 lab4_qlearning% READ THIS : After weeks of working, I still couldnt get my lab work properly, or rather far from%working. However, Ive decided to work on the 3 by 2 gridworld example which you(the TA)%presented during the lab, and I implemented the GUI(or sort of) into your coding example. Hope if I%can still get some credits for this.% Consider the following gridworld:% + - - - - + - - - - + - - - - +% |         |         |         |% |  START  |    2    |    3    |% |    1    |         |         |% + - - - - + - - - - + - - - - +% |         |         |         |% |    4    | KEEPOUT% CSE 516 Machine Learning% Q - learning coding example |  GOAL.  |% |         |    5    |    6    |% + - - - - + - - - - + - - - - +% transitions. 0 means invalid transition.T = [ 0 4 0 2 ;       0 5 1 3 ;      0 6 2 0 ;      1 0 0 5 ;      2 0 4 6 ;      3 0 5 0 ];% reward in single vector since we have the transition matrix defined separatly% and R(s'|s,a) is equivalent to R(s')R = [-1,-1,-1,-1,-10,10]; % Q matrixQ = zeros(6,4);% discount rate : gamma gamma = 0.8;axis([-0.5,3.5,-0.5,2.5]);hold on;plot([0,0],[0,2],'b-','linewidth',2);plot([1,1],[0,2],'b-','linewidth',2);plot([2,2],[0,2],'b-','linewidth',2);plot([3,3],[0,2],'b-','linewidth',2);% CSE 516 Machine Learning% Q - learning coding exampleplot([0,3],[0,0],'b-','linewidth',2);plot([0,3],[1,1],'b-','linewidth',2);plot([0,3],[2,2],'b-','linewidth',2);for i=1:10  ss = 1 ; % start from the start state  curr = ss; % current state    while curr ~= 6    % pick a random action, and end up in a random valid state.    possibiles = T(curr,:);    [action, nxt_state] = random_choice(possibiles);    % immediate reward:    imm_r = R(nxt_state);        % this block is to calculate gamma*max(Q'(s',a'))    future_rewards = [];    future_states = T(nxt_state,:);    for a = 1:length(future_states)      expected_future_state = future_states(a);      if expected_future_state ~= 0        future_rewards = [future_rewards Q(nxt_state, a)];      end    end    discounted_max_future_reward = gamma*max(future_rewards);    % end of block        % update Q value:       q_value = imm_r + discounted_max_future_reward;    Q(curr,action) = q_value;    fprintf('currentstate %d', curr)    [x1,y1] = getXY(curr);    %[x2,y2] = getXY(nxt_state);    mydraw(x1,y1,1)        curr = nxt_state;    end   display(Q);  pause(1);end                                    